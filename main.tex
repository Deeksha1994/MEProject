
%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx} %package to manage images
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\title{\LARGE \bf
A MST-KNN based Multiclass Classification By Using Class Distance Measure For Similarity Clustering Of Classes.}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Deeksha Punase$^{1}$ and Surendra Gupta$^{2}$% <-this % stops a space
% <-this % stops a space
\thanks{$^{1}$Deeksha Punase is currently pursuing master of engineering in Computer Engineering from S.G.S.I.T.S., Indore, India.}%
\thanks{$^{2}$Surendra Gupta is currently working as an Assistant Professor in Computer Engineering Department at S.G.S.I.T.S., Indore, India.}%
}


\begin{document}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Multiclass Classification is an integral part of classification in data mining.Sentiment analysis, text classification, hand-written recognition,traffic sign recognition are the applications where multiclass classification is useful.The existing methods for multiclass classification are One-vs-One (OvO), One-vs-All (OvA), DAGSVM, Binary tree SVM(BTS).Binary tree SVM were also extented to multi-way tree where more than two child nodes can be created.The proposed approach uses existing multi-way tree structure.A better class distance measure is used to find similar classes based on within and between class distance.A MST-KNN approach is applied along with class distance measure which results in better grouping/clustering of classes and avoid data imbalanced problem of One-vs-All method.The proposed approach is compared with other existing methods and experiments were performed on many data sets which demonstrates that our method increases overall accuracy of classification where number of instances and classes are high. 
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Data mining is extraction of useful information or knowledge from large volume of data.These large volume of data is explored and analyzed so as to discover some meaningful patterns and useful information.It can be used in any of the applications such as Fraud Detection, Market Analysis, Production Control, Customer Retention.Three of the major data mining techniques are regression, classification and clustering.Classification is one of those methods which makes the analysis of very large data samples effective.
\par
\par 
Classification is a process that assigns items in a collection to target categories or classes.Classification predicts a certain output based on some input.For predicting the correct outcome the algorithm takes a training set containing a set of attributes/features as input alongwith the target attribute in which the samples are to be classified.Algorithm discovers the relationships between the attributes that helps in predicting the outcome.Then the algorithm is provided with an unseen data set known as testing(prediction) set.The testing data set contains same attributes set and excludes the target attribute.The algorithm completely analyses the input and produces a prediction.
\par 
There are two types of classification problems – Binary Classification and Multiclass Classification.Binary classification is the task of classifying the elements of a given set into two classes on the basis of a classification function that assigns its class to each data item.
\par
Multiclass Classification which involves assigning an element to one of several classes (i.e, multiple classes). Each training data point belongs to one of N different classes.The goal is to construct a function or classifier which will predict the class correctly when a new data point is given.
\par
SVMs were initially designed for binary classification problem.SVM have been applied in many fields such as image classification ,handwritten digit and character recognition,and some other classification.In real world, there are mainly multiclass classification problems, so to accurately classify data samples into large number of classes is a challenging task.In multiclass classification, each data sample belongs to one of N classes.
There are two approaches to solve multiclass SVM :
First, considering all data at once i.e, constructing N SVMs decision functions for N class problem.Optimizing N binary SVMs problems into one big optimal formulation.Although this approach is difficult to implement.Second, dividing the multiclass problem into multiple binary problems.Training those multiple binary problems by original SVM to get several SVMs, then combining those SVMs to do the classification.Second approach has several methods such as One-against-One (OAO)[5], One-against-All (OAA)[5], directed acyclic graph SVM (DAGSVM)[5] and binary tree SVM[13](BTS).OAO method divides original problem into multiple binary problems, one classifier is needed to train each pair of class.The number of binary classifiers needed is N(N-1)/2, for a N class problem.Drawback of this method is that as the number of classes increases, number of classifiers are also increased.In, OvA each class is trained against the rest (N - 1) classes, where total N classifiers are required for N class problem.Drawback of this method is data imbalance problem.
\par
Decision Directed Acyclic Graph (DDAG) training phase is same as that of OvO method.The number of classifiers required for N class problem in one classification is equal to N(N-1)/2.The drawback is same as that of OAO method.Binary tree of SVM (BTS) builds a binary tree structure.In each non-leaf node of the binary tree, to train one SVM classifier two classes are randomly selected.For each child node, the classifier divides the data into two sub-nodes clusters.This process is continued until each leaf node contains only one class data samples.
Later, Divide-by-2 (DB2) approach was also proposed, which builds a binary tree structure.The approach splits classes into two clusters based on grand mean or k-means algorithm.

Binary tree structure is further extended to Multi-way tree structure where multiple child nodes(\textgreater2) are created and classes are grouped together on the basis of similarity.On each node of the multi-way tree, classifiers are applied on each group of classes.These methods has some limitations in terms of grouping of classes, total number of classifiers and training time.Therefore, new approach for multi-way tree structure is needed which accurately classifies the data samples considering drawbacks of all other methods. 
\vspace{0.5cm}
\par
Initially, in our proposed approach, best features are selected using Information Gain feature selection method.The second next step is similar classes clustering, where similar classes are grouped into different clusters.
We proposed a similarity clustering method, which evaluates similarity between classes using a class distance measure which considers the distribution(within class,prior probability,total number of samples in each class) and distance(between class) of the data in feature space.The class distance measure includes both within class and between class distance between classes.Based on class distance measure MST-KNN approach is applied which further leads to grouping of similar classes.The third step is training of groups,where OvA is applied to train similar classes groups.Then, in next step, one node gets divided into  multiple sub-nodes where each node represents a single group.We repeat the whole process until samples of each class reaches the leaf-node, the multi-way tree gets constructed.The experiments performed on various data sets shows that our proposed method performs well than other existing methods in terms of overall accuracy,  training time and number of classifiers required in one classification.

\par
Organization of paper is as follow. In Section 2, basics of SVMs is explained. In Section 3, proposed approach is explained in detail where we focus on how to perform similar classes clustering approach.In Section 4, we perform comparison based experiments on various multiclass classification data-sets.Section 5, concludes our proposed approach.


\section{SVM}

A support vector machine[4] is a supervised classification method.It is used for classification and regression both for binary and multi-class problems.It supports text mining, pattern recognition, nested data problems.There are mainly two kinds of problem : \begin{itemize}
\item Linearly separable, 
\item Non-Linearly separable problem.
\end{itemize}
Linearly separable problem are separated by a straight hyperplane whereas Non-Linearly separable problem cannot be separated.Non-linearly separable problem are solved by transforming the data to higher dimensional feature space from input space.SVM uses a linear decision boundary that maximizes the minimum distance between two class groups.
\par
For binary class problem, given training data samples:(X\textsubscript{1}, Y\textsubscript{1}), (X\textsubscript{2}, Y\textsubscript{2}), ... , (X\textsubscript{n}, Y\textsubscript{n}), where  n is the total number of samples, X is the input space, Y are the class labels where Y $\in$ \{-1, 1\}.

\hspace{2.5cm}f(X) = WX + B \hspace{3cm}(1) \\
where W and B are parameters of the model.
The optimization problem can be formulated as \\

$\hspace{0.5cm} min\frac{1}{2}W\textsuperscript{T}W\hspace{1cm}
s.t.:Y_{i}(WX_{i}+B) >=1 \hspace{0.65cm}(2)$ \\

The solution of optimization problem is given by Lagrangian 
\[\hspace{1cm}L_{P} = \frac{1}{2}||W||\textsuperscript{2} - \sum_{i=1}^{N} A_{i}[Y_{j}(WX_{i}+B) -1]  \hspace{0.9cm}(3) \]
When A\textsubscript{i} are solved, W can be calculated as 
\[\hspace{2.5cm}W = \sum_{i=1}^{N} A_{i}Y_{i}X_{i} \hspace{3cm}(4)\]   
For the nonlinear problem, by using kernel function ψ(.) the problems can be solved. 
\[\hspace{1cm}max L_{D} = \sum_{i=1}^{N} A_{i} - \frac{1}{2}\sum_{i,j}^{N} A_{i}A_{j}Y_{i}Y_{j}k(X_{i}\,X_{j}) \hspace{0.5cm}(5)\] 
$where\hspace{0.2cm}[k(X_{i}\,X_{j})= exp(\Gamma||X_{i}-X_{j}||\textsuperscript{2}_]$, which is the radial basis function (RBF) kernel function used in the proposed approach.


\section{Proposed Approach}

\subsection{Feature Selection}
\par 
Feature selection[2] algorithm focuses on selecting relevant features for classification. A filter based feature selection method used in this approach. It's working is as : In the first step, it ranks features based on some criteria, each feature is ranked independently of the feature space. In the second step, the feature with top ranking is chosen.The filter based feature selection method used is :
\par
\textbf{Information Gain}: Information gain[2] measures the amount of information in bits about the class prediction, if the only information available is the presence of a feature and the corresponding class distribution.Given, T\textsubscript{X} is the training samples set, x\textsubscript{i} the vector of i\textsuperscript{th} variables in this set, T\textsubscript{xi=v}/T\textsubscript{X} the fraction of examples of the i\textsuperscript{th} variable having value v : 
\newline
% \[\hspace{1cm}S_{k} = \frac{\sum_{i=1}^{K} n_{i}(\mu_{ij}-\mu_{i})\textsuperscript{2}}{\sum_{i=1}^{K}n_{j}\rho\textsuperscript{2}_{ij}} \hspace{2cm}(3.1) \] \\  

\hspace{1cm}IG(T\textsubscript{X}, x\textsubscript{i}) = H(T\textsubscript{X}) − $  \sum_{v=values(x\textsubscript{i})}^\frac{|T\textsubscript{x\textsubscript{i}=v}|}{|T\textsubscript{X}|}$ H(T\textsubscript{x\textsubscript{i}=v})\hspace{0.5cm}(6)\\ 
with entropy :
\newline
\par
\hspace{1cm}H(T) = $−p_{+}(T) log_{2} p_{+}(T) − p_{−}(T) log_{2} p_{−}(T)$\hspace{1.25cm}(7)\\
\newline
$p_{±}(T)$ is the probability of the training sample in the set T to be of the positive/negative class.Individual feature score is calculated and the highest feature according to their rank is selected for the next steps of classification.

\subsection{Similar Classes Clustering}

\subsubsection{Similarity Evaluation}
Similarity measure[3] is defined as the distance between various data samples.Similarity represents the relationship between two data samples i.e, how alike they are whereas dissimilarity is the inverse of similarity.There are many existing methods which can be used to evaluate the difference between classes in terms of distance like Cosine, Euclidean distance , Pearson's corelation, Manhattan distance. Jaccard Distance.All these methods performs well in 2-dimension or 3-dimension space.However, there are many high dimension datasets, so to represent the difference between classes in the feature space we should consider both distribution and distance of data simultaneously.In proposed approach, similarity is obtained through  distance(between class distance) and distribution(within class, prior probability, number of samples) of data samples.Lesser the distance value more similar are the classes and vice-versa.Class Distance measure used in proposed approach is shown in equation (11).The measure details are as : 
\newline
1) $D_{ij}$ is the within class distance for $i_{th}$ class.

\[\hspace{2cm}D_{i} = \frac{p_{i}}{n_{i}}(x_{s}-\frac{1}{n_{i}}\sum_{i=1}^{n_{i}}x_{i})^{2}\hspace{2.2cm}(8)\]\\  
where s = 1...n(total number of samples in $i_{th}$ class).
\newline
2) $M_{ij}$ is the between class distance between $i_{th}$ class and $j_{th}$ class in feature space.Squared euclidean distance is calculated between classes mean.
\par
\[M_{ij} = ((\mu_{i})-(\mu_{j}))^{2} = (\frac{1}{n_{i}}\sum_{i=1}^{n_{i}}x_{i} - \frac{1}{n_{j}}\sum_{j=1}^{n_{j}}x_{j})^{2}\hspace{0.7cm}(9)\] \\
The Distance formula is summed up as;
\[Class\_Dist[i,j] = \frac{\sum_{j=1}^{C}\sum_{i=1}^{n}\frac{p\textsubscript{j}}{n\textsubscript{j}}*(x_{i}-\mu_{j})\textsuperscript{2}}{((\mu_{i})-(\mu_{j}))^{2}}\hspace{1cm}(10) \] \\  
\par 
where $p_{j}$ is the prior probability of $j^{th}$ class , $n_{j}$ is the total number of samples in $j^{th}$ class, $x_{i}$ are the data points which belong to the respective $j^{th}$ class, $\mu_{i}$,  $\mu_{j}$ are the overall mean of $i^{th}$ and $j^{th}$ class respectively .The Class\_Dist[i,j] represents a similarity/dissimilarity value which considers both within and between class distance between every pair of class.The equation mentioned in (11) is represented as ;

\[Class\_Dist[i,j]=\frac{Within-Class-Distance}{Between-Class-Distance}\hspace{0.5cm}(11)\] \\
A Matrix is formed using Class\_Dist[i,j] values between every pair of classes.The values of matrix represents distance between every pair of class.A symmetric matrix is obtained as the distance of a class A to other class B will remain same as distance of B to A.Lesser the distance more similar are the classes.

\subsubsection{MST-KNN Graph} 
MST-KNN[8] [9] approach is used for obtaining better classes clusters.
\begin{itemize}
\item\textbf{Minimum spanning tree}
\par
The distance matrix is converted to a complete undirected graph.The graph contains classes as nodes and the distance values as the graph edges.Apply Minimum Spanning tree[12] algorithm on the graph obtained so as to cluster similar classes into different groups in the next step.Kruskal's minimum spanning tree algorithm is used in the proposed approach.  
\item\textbf{K-Nearest Neighbor}
\par
KNN algorithm is applied on the distance matrix so as to obtain a KNN graph.It finds out its nearest neighbor by using the distance matrix.
\item\textbf{Common edges}
\par
MST and KNN graph edges are compared with each other and common edges are selected for grouping i.e, the classes which are found in the common edges are grouped in clusters.And Multi-tree child nodes are created based on the number of clusters. 
\item\textbf{Threshold edge deletion}
\par
Threshold edge deletion[1] step is applied in sub-clustering process where clusters are further divided into different clusters. If same clusters are obtained in sub-clustering process after applying both MST and KNN common edge step then edge deletion is done.Minimum spanning tree is generated for the respective classes and edge deletion is applied on the MST.To obtain similar classes group, delete the edge with value greater than $\delta,where$ 
\par
\[\hspace{2cm}\delta = \frac{\Delta W}{(N-1)}\hspace{3cm}(12)\] \\

\par 
where W is the total weight of the spanning tree and N is the total number of classes.

\item\textbf{SVM-One-Against-All}
\par
Similar classes are clustered into different groups and SVM One-Against-All is applied on each node.Out of N classes, "k" groups are created where k\textgreater1, each group contains samples of M classes where M represents number of classes in a group (M\textgreater=1).For training k groups k classifiers are needed.If k size equals to 2, then only one classifier is needed.When OvA is applied for groups training, the ratio between positive and negative samples is found to be closer to 1.
\end{itemize}
The classification algorithm is explained below in detail:
\newline
\textit{\textbf{Procedure 1}}: Add training data samples to the root node.
\newline \\
\textit{\textbf{Procedure 2}}: Group similar classes into different clusters by similarity clustering method, "k" groups will be created in N class problem.
\par \textit{\textbf{Procedure 2.1}}: Add feature selection method to obtain best features.
\par \textit{\textbf{Procedure 2.2}}: Calculate Within class and Between class distance to obtain a class distance matrix.
\par \textit{\textbf{Procedure 2.3}}: Based on that matrix a complete connected undirected graph is generated.
\par \textit{\textbf{Procedure 2.4}}: Apply Minimum Spanning Tree algorithm on that graph to get a Spanning tree. 
\par \textit{\textbf{Procedure 2.5}}: Apply KNN algorithm on the distance matrix obtained to obtain a KNN graph.
\par\textit{\textbf{Procedure 2.6}}: Compare both MST and KNN edges for grouping common edges.Groups of similar classes are formed in this step. \\
\newline
\textit{\textbf{Procedure 3}}: Train obtained groups by One Against All approach.In this process, k classifiers are constructed.
\newline \\
\textit{\textbf{Procedure 4}}: The current node is divided into multiple sub-nodes.If the current node contains "k" groups, then "k" sub-nodes will be formed.
\par \textit{\textbf{Procedure 4.1}}: If k\textgreater1 then divide the node by repeating the same procedure 2.
\par \textit{\textbf{Procedure 4.2}}: If k=1 i.e, sub-node is same as current node then perform threshold edge deletion. 
\par \textit{\textbf{Procedure 4.3}}: Compute MST for classes of that group and then delete edges whose distance value is greater than threshold value.
\newline \\
\textit{\textbf{Procedure 5}}: The new sub-nodes created are considered to be as multi-way tree new node. For one complete classification, \textbf{procedure 2} to \textbf{procedure 5} is continued until return.
\newline
The structure of hierarchical multi-way tree is shown below using a 10 class pendigits problem ;
\begin{figure}[h!] 
\hspace{0.2cm}\includegraphics[scale=1,height=3in,width=3.5in]{pendigits_tree.jpg}
\caption{Example: Pendigits(10 class) problem}  
\end{figure}
\par
Initially all 10 classes(0 to 9) are present at root node.After applying all the steps, a multi-way tree is obtained.The tree represents different groups as Group A : 4, 6; Group B : 5, 0, 9, 8 ; and the rest classes 7, 2, 3, 1 as individual separate node.The root node contains following information: data samples, similarity clustering result and six classifiers each for one group. After training, six sub-nodes 1,2,3,4,5 and 6 are created. Each contains the samples in one group of the similarity clustering result.The data samples gets classified in top-down fashion.Each testing sample starts from the root node, classifier classifies it into sub-node.This process is continued until sample reaches to the leaf node.

\section{Experiments}

In this section, we present experimental results.The algorithm is implemented in
R language using Rstudio on a PC with intel core i5 CPU (2.5GHz), 4GB RAM.RStudio is a open-source and free Integrated Development Environment for R.R is a programming language for statistical computing and graphics.Our proposed method is compared with the following methods:
\begin{itemize}
\item One-against-rest.
\item One-against-one.
\item Binary Tree SVM (BTS).
\item Hierarchical SVM-MST method.
\end{itemize}
Our proposed method is compared with the above mentioned methods in respect with accuracy.

\subsection{Experiments and data result}
For experimentation, Nine multiclass datasets are taken from UCI Machine Learning Repository.A brief description of the datasets is summarized in Table I.Samples are divided into 70-30 ratio in which 70\% samples belong to Training samples and 30\% samples belong to Testing samples.We considered K-fold testing method in which K is 10.The datasets are divided into three categories : First category is High range, where instances and classes are high, Second category is Mid range in which instances are from between 1000 and 10000 and classes low to high, Third category is Low range where instances are less i.e, less than 1000 and classes from low to high.

\subsection{Analysis of experiment results}
Table I describes about dataset used in the experiment.Tables II, III, IV describes about k-fold cross-validation results on all three categories where k = 10.Table V represents overall accuracy results one from each category.The Overall accuracy is evaluated using True Positive and True Negatives.\\
\par Overall Accuracy = $\frac{TP + TN}{TP + TN +FP + FN}$\hspace{2cm}(13) \\
\newline
We have N classes, then confusion matrix would be an N × N matrix, where row represents True Classes and Columns represents Predicted classes. Each element {i,j} of the matrix would be the number of items with True Class i that were classified as being in Class j.
\par

\begin{table}[h!]
  \centering
  \caption{Datasets used for Experimentation}
 
  \label{tab:table1}
  \begin{tabular}{|c|c|c|c|c|c|}
   \hline
   \textbf{Dataset} & \textbf{Instances}  & \textbf{Features} & \textbf{Class} 
   & \textbf{Training} & \textbf{Testing} \\
   \hline
   Letter & 20000 & 18 & 26  & 13334 & 6666 \\
    \hline
   Pendigits &10992 & 17 & 10 & 7328 & 3664\\
    \hline
   Satimage & 6435  & 36 & 6 & 4290 & 2145\\
   \hline
   Vowel & 990  & 10 & 10 & 660 & 330\\
   \hline
   Optdigits & 5620 & 64 & 10  & 3747 & 1873\\
   \hline
   Pageblocks & 5473 & 10 & 5 & 3649 & 1824\\
   \hline
   Segment & 2310 & 21 & 7 & 1540 & 770 \\
   \hline
   Soybean & 683  & 35 & 19 & 456 & 227\\
   \hline 
   KDD\_synthetic & 600 & 61 & 5 & 400 & 200\\
   \hline
   Vehicle & 846 & 19 & 4 & 564 & 282 \\
   \hline
  \end{tabular}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Category I (\textbf{High Range}) datasets accuracy}
  \label{tab:table2}
   \begin{tabular}{|c|c|c|c|c|c|c|c|}
   \hline
   \textbf{Dataset} & \textbf{Instances} & \textbf{Classes} & \textbf{Training} & \textbf{Testing} & \textbf{Accuracy}\\
   \hline
   Letter & 20000 & 26  & 13334 & 6666 & 99.83\\
    \hline
   Pendigits &10992 & 10 & 7328 & 3664 & 99.60 \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Category II (\textbf{Mid Range}) datasets accuracy}
  
  \label{tab:table3}
   \begin{tabular}{|c|c|c|c|c|c|c|c|}
   \hline
   \textbf{Dataset} & \textbf{Instances} & \textbf{Classes} & \textbf{Training} & \textbf{Testing} & \textbf{Accuracy}\\
   \hline
   Satimage & 6435 & 6 & 4290 & 2145 & 99.19\\
   \hline
   Pageblocks & 5473 & 5 & 3649 & 1824 & 97.93 \\
   \hline
   Optdigits & 5620 & 10  & 3747 & 1873 & 95.19 \\
   \hline
   Segment & 2310 & 7 & 1540 & 770 & 97.99\\
   \hline
  \end{tabular}
\end{table}
  
\begin{table}[h!]
  \centering
  \caption{Category III (\textbf{low Range}) datasets accuracy}
  \label{tab:table4}
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
   \hline
   \textbf{Dataset} & \textbf{Instances} & \textbf{Classes} & \textbf{Training} & \textbf{Testing} & \textbf{Accuracy}\\
   \hline 
   Vowel & 990  & 10 & 660 & 330 & 98.60 \\
   \hline
   KDD\_syn & 600 & 5 & 400 & 200 & 96.87  \\
   \hline
   Soybean & 683 & 19 & 456 & 227 & 90.53 \\
   \hline 
   Vehicle & 846 & 4 & 564 & 282 & 80.97 \\
   \hline
  \end{tabular}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Comparison with other approaches}
  \label{tab:table5}
	\begin{tabular}{|c|c|c|c|c|c|}
   \hline
   \textbf{Dataset} &  \textbf{OAR}  &  \textbf{OAO} &  \textbf{BTS} &	\textbf{SVM-MST} &  \textbf{Proposed} \\
    &   &  &  &	 &  \textbf{Method} \\    
	\hline
	Letter(HR)  & 97.96  & 97.88 & 97.91 & 98.00 & 99.83\\
	\hline
 	Pendigits(HR)  & 98.26 & 97.40 & 98.11 & 98.21 & 99.60 \\ 
	\hline
    Satimage(MR)  & 91.31 & 91.74  & 91.25 & 94.20 & 99.19 \\ 
	\hline
	Soybean(LR)  &  94.21 & 93.59 & 92.87 & 94.73 & 90.53\\
	\hline
\end{tabular}
\end{table}
Figure 2 represents the accuracy comparison bar chart between existing methods and proposed approach.
\begin{figure}[h!] 
\hspace{0.2cm}\includegraphics[scale=0.5,height=2.5in,width=3.3in]{chart.JPG}
\caption{Accuracy Comparison Bar Chart}  
\end{figure}
It is clear from the bar chart that letter, pendigits and satimage datasets from high and mid range category has increase in accuracy whereas soybean dataset from low range category shows decrease in accuracy than all other methods. 

\section{CONCLUSIONS}
In this paper, we proposed a new approach using multi-way tree and similarity clustering method using class distnace measure for multiclass classification.The experiments shows that the performance of proposed approach is better than existing approaches for high and mid range datasets whereas for low range dataset where instances are less and classes are more it doesn't provide better accuracy result.The proposed approach used better similarity clustering method considering within class, between class distance between each pair of classes and distribution of samples in each class.MST-KNN together works efficiently and provided the best grouping possible.As the proposed similarity clustering method trains groups rather than classes it can reduce the data imbalanced problem of OAA method.  


\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{99}

\bibitem{} Chao DONG, Bo ZHOU and Jinglu HU { ,“A Hierarchical SVM Based Multi-class Classification by Using Similarity Clustering ”}, International Joint Conference on Neural Networks (IJCNN), July 2015. 

\bibitem{} Tang, J., Alelyani, S., and Liu, H. { , "Feature selection for classification: a review"}, in Aggarawal, C.C. Ed.2014.

\bibitem{}  Anil Kumar Patidar, Jitendra Agrawal, Nishchol Mishra { ,“Analysis of Different Similarity Measure Functions and their Impacts on Shared Nearest Neighbor Clustering Approach” }, International Journal of Computer Applications, February 2012.

\bibitem{} Chih-Wei Hsu and Chih-Jen Lin { ,“A Comparison of Methods for Multi-class Support Vector Machines”},Department of Computer Science and Information Engineering National Taiwan University ,2001.

\bibitem{} Mohamed Aly  { ,“Survey On Multiclass Classification Method ”},November 2005.

\bibitem{} S. Kumar, J. Ghosh, M.M. Crawford { ,"Hierarchical fusion of multiple classifiers for hyperspectral data analysis, Pattern Analysis \& Applications, 5:210-220 "}, 2002.

\bibitem{} V. Vural, J.G. Dy { ,"A hierarchical method for multi-class support vector machines. In Proceedings of the Twenty-First International Conference on Machine Learning, 105-112 "}, 2004.

\bibitem{} Ahmed Shamsul Arefin, Carlos Riveros, Regina Berretta, Pablo Moscato { ,“ kNN-MST-Agglomerative: A Fast and Scalable Graph-based Data Clustering Approach on GPU.”}, 7th International Conference on Computer Science \& Education (ICCSE 2012) Melbourne, Australia, July 14-17, 2012.

\bibitem{} Ahmed Shamsul Arefin, Carlos Riveros, Regina Berretta, Pablo Moscato { ,“The MST-kNN with Paracliques”}, The Priority Research Centre for Bioinformatics Biomarker Discovery and Information-based Medicine University of Newcastle, February 2015.

\bibitem{} Xiaochun Wang, Xiali Wang and D. Mitchell Wilkes {, “A divide-and conquer
approach for minimum spanning tree-based clustering,”} IEEE  Trans. Knowledge and Data Engg., vol. 21, 2009.

\bibitem{} Volkan Vural, Jennifer G. Dy {,"A Hierarchical Method for Multi-Class Support Vector Machines"}, 2004.

\bibitem{} Prasanta K. Jana{,"An Efficient Minimum Spanning Tree based Clustering Algorithm"}, International Conference on Methods and Models in Computer Science, 2009.

\bibitem{}Ben Fei and Jinbai Liu{,"Binary Tree of SVM: A New Fast Multiclass
Training and Classification Algorithm"},IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 3, MAY 2006.

\bibitem{} https://www.rstudio.com/online-learning/R

\bibitem{} {,"Introduction to R"} https://www.datacamp.com

\bibitem{} https://archive.ics.uci.edu/ml/datasets.html

\bibitem{} https://cran.r-project.org/web/packages/index.html


\end{thebibliography}




\end{document}
